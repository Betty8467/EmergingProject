{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2b5ee574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "842b104d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>542644</th>\n",
       "      <th>548869</th>\n",
       "      <th>93219</th>\n",
       "      <th>534737</th>\n",
       "      <th>36820</th>\n",
       "      <th>542661</th>\n",
       "      <th>10066</th>\n",
       "      <th>542456</th>\n",
       "      <th>24820</th>\n",
       "      <th>...</th>\n",
       "      <th>543880+543332+542784+530997+530785</th>\n",
       "      <th>535034+535038+542800+543348+543896</th>\n",
       "      <th>543924+543376+542828+535086</th>\n",
       "      <th>542832+542696+543380+543928</th>\n",
       "      <th>543932+542700+542836+543384</th>\n",
       "      <th>547724+543360+547728+543908+547720+542812+547716+531033</th>\n",
       "      <th>543888+542792+543340+531025</th>\n",
       "      <th>543372+543368+543920+543916+542692+542688+542824+542820</th>\n",
       "      <th>543364+543912+542684+542816</th>\n",
       "      <th>8852013+8852220+10743960+10778824+10778836+10743901+10778899+10779132</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19910101</td>\n",
       "      <td>216.0400</td>\n",
       "      <td>404.7017</td>\n",
       "      <td>146.25000</td>\n",
       "      <td>281.3800</td>\n",
       "      <td>155.250</td>\n",
       "      <td>214.4300</td>\n",
       "      <td>268.750</td>\n",
       "      <td>204.9100</td>\n",
       "      <td>312.130</td>\n",
       "      <td>...</td>\n",
       "      <td>33122.130000</td>\n",
       "      <td>45839.530000</td>\n",
       "      <td>52842.860000</td>\n",
       "      <td>48596.370000</td>\n",
       "      <td>49887.600000</td>\n",
       "      <td>60081.570000</td>\n",
       "      <td>39845.120000</td>\n",
       "      <td>52658.090000</td>\n",
       "      <td>31296.190000</td>\n",
       "      <td>13.03000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19910201</td>\n",
       "      <td>149.0800</td>\n",
       "      <td>421.4927</td>\n",
       "      <td>115.66667</td>\n",
       "      <td>276.2600</td>\n",
       "      <td>95.750</td>\n",
       "      <td>140.6300</td>\n",
       "      <td>280.000</td>\n",
       "      <td>136.6500</td>\n",
       "      <td>307.280</td>\n",
       "      <td>...</td>\n",
       "      <td>56664.570000</td>\n",
       "      <td>73207.630000</td>\n",
       "      <td>89338.320000</td>\n",
       "      <td>80358.930000</td>\n",
       "      <td>85764.820000</td>\n",
       "      <td>81404.760000</td>\n",
       "      <td>68741.690000</td>\n",
       "      <td>80129.440000</td>\n",
       "      <td>54080.590000</td>\n",
       "      <td>13.15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19910301</td>\n",
       "      <td>119.0500</td>\n",
       "      <td>368.1324</td>\n",
       "      <td>86.33333</td>\n",
       "      <td>190.7900</td>\n",
       "      <td>80.800</td>\n",
       "      <td>107.5100</td>\n",
       "      <td>199.200</td>\n",
       "      <td>106.0300</td>\n",
       "      <td>226.210</td>\n",
       "      <td>...</td>\n",
       "      <td>35121.180000</td>\n",
       "      <td>48163.420000</td>\n",
       "      <td>55941.780000</td>\n",
       "      <td>51293.410000</td>\n",
       "      <td>52934.020000</td>\n",
       "      <td>52744.530000</td>\n",
       "      <td>42298.800000</td>\n",
       "      <td>54990.750000</td>\n",
       "      <td>33230.870000</td>\n",
       "      <td>13.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19910401</td>\n",
       "      <td>121.9400</td>\n",
       "      <td>387.0115</td>\n",
       "      <td>79.66667</td>\n",
       "      <td>200.2700</td>\n",
       "      <td>77.000</td>\n",
       "      <td>110.7000</td>\n",
       "      <td>191.375</td>\n",
       "      <td>108.9800</td>\n",
       "      <td>235.200</td>\n",
       "      <td>...</td>\n",
       "      <td>19576.480000</td>\n",
       "      <td>30092.690000</td>\n",
       "      <td>31844.400000</td>\n",
       "      <td>30321.090000</td>\n",
       "      <td>29244.860000</td>\n",
       "      <td>36829.640000</td>\n",
       "      <td>23218.850000</td>\n",
       "      <td>36851.860000</td>\n",
       "      <td>18186.700000</td>\n",
       "      <td>13.51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19910501</td>\n",
       "      <td>125.7900</td>\n",
       "      <td>462.6200</td>\n",
       "      <td>82.80000</td>\n",
       "      <td>199.6400</td>\n",
       "      <td>78.600</td>\n",
       "      <td>114.9600</td>\n",
       "      <td>197.200</td>\n",
       "      <td>112.9100</td>\n",
       "      <td>234.610</td>\n",
       "      <td>...</td>\n",
       "      <td>34175.280000</td>\n",
       "      <td>47063.810000</td>\n",
       "      <td>54475.450000</td>\n",
       "      <td>50017.240000</td>\n",
       "      <td>51492.530000</td>\n",
       "      <td>58368.520000</td>\n",
       "      <td>41137.780000</td>\n",
       "      <td>53887.000000</td>\n",
       "      <td>32315.440000</td>\n",
       "      <td>13.05000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>20230901</td>\n",
       "      <td>684.4500</td>\n",
       "      <td>576.0000</td>\n",
       "      <td>568.45000</td>\n",
       "      <td>1024.2000</td>\n",
       "      <td>603.600</td>\n",
       "      <td>711.9000</td>\n",
       "      <td>1054.800</td>\n",
       "      <td>659.7000</td>\n",
       "      <td>1022.900</td>\n",
       "      <td>...</td>\n",
       "      <td>30863.310778</td>\n",
       "      <td>25898.544058</td>\n",
       "      <td>48481.681142</td>\n",
       "      <td>45368.326005</td>\n",
       "      <td>50994.194898</td>\n",
       "      <td>55121.678414</td>\n",
       "      <td>32958.539815</td>\n",
       "      <td>54409.884701</td>\n",
       "      <td>36524.003032</td>\n",
       "      <td>11.51250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>20231001</td>\n",
       "      <td>660.3750</td>\n",
       "      <td>656.1875</td>\n",
       "      <td>524.37500</td>\n",
       "      <td>970.3125</td>\n",
       "      <td>556.625</td>\n",
       "      <td>700.1250</td>\n",
       "      <td>1014.750</td>\n",
       "      <td>664.6875</td>\n",
       "      <td>973.000</td>\n",
       "      <td>...</td>\n",
       "      <td>42638.283715</td>\n",
       "      <td>50128.701163</td>\n",
       "      <td>63854.584045</td>\n",
       "      <td>65398.069490</td>\n",
       "      <td>70789.307587</td>\n",
       "      <td>65391.097779</td>\n",
       "      <td>45001.265013</td>\n",
       "      <td>72728.810396</td>\n",
       "      <td>49016.415023</td>\n",
       "      <td>11.68875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>20231101</td>\n",
       "      <td>629.7500</td>\n",
       "      <td>688.1875</td>\n",
       "      <td>489.75000</td>\n",
       "      <td>919.5625</td>\n",
       "      <td>521.875</td>\n",
       "      <td>713.3125</td>\n",
       "      <td>936.125</td>\n",
       "      <td>686.7500</td>\n",
       "      <td>880.875</td>\n",
       "      <td>...</td>\n",
       "      <td>61926.342563</td>\n",
       "      <td>67212.546340</td>\n",
       "      <td>87067.677940</td>\n",
       "      <td>82487.096193</td>\n",
       "      <td>89553.720260</td>\n",
       "      <td>88069.251739</td>\n",
       "      <td>65533.770453</td>\n",
       "      <td>87299.379274</td>\n",
       "      <td>60576.883930</td>\n",
       "      <td>11.88625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>20231201</td>\n",
       "      <td>588.4500</td>\n",
       "      <td>589.7000</td>\n",
       "      <td>494.65000</td>\n",
       "      <td>868.7500</td>\n",
       "      <td>503.700</td>\n",
       "      <td>681.8500</td>\n",
       "      <td>881.800</td>\n",
       "      <td>615.7000</td>\n",
       "      <td>804.650</td>\n",
       "      <td>...</td>\n",
       "      <td>52011.115747</td>\n",
       "      <td>59195.962123</td>\n",
       "      <td>70954.699113</td>\n",
       "      <td>67922.393093</td>\n",
       "      <td>71746.666697</td>\n",
       "      <td>72573.820342</td>\n",
       "      <td>54569.016270</td>\n",
       "      <td>73118.989011</td>\n",
       "      <td>47718.064695</td>\n",
       "      <td>11.77250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>20240101</td>\n",
       "      <td>585.4375</td>\n",
       "      <td>509.5000</td>\n",
       "      <td>459.56250</td>\n",
       "      <td>845.3125</td>\n",
       "      <td>493.875</td>\n",
       "      <td>657.5625</td>\n",
       "      <td>890.375</td>\n",
       "      <td>611.7500</td>\n",
       "      <td>833.125</td>\n",
       "      <td>...</td>\n",
       "      <td>44979.617637</td>\n",
       "      <td>53176.273842</td>\n",
       "      <td>76490.177393</td>\n",
       "      <td>73701.441735</td>\n",
       "      <td>77820.435563</td>\n",
       "      <td>76757.290364</td>\n",
       "      <td>50301.784130</td>\n",
       "      <td>77983.329021</td>\n",
       "      <td>51288.683635</td>\n",
       "      <td>11.73250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows Ã— 464 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    542644    548869      93219     534737    36820    542661  \\\n",
       "0      19910101  216.0400  404.7017  146.25000   281.3800  155.250  214.4300   \n",
       "1      19910201  149.0800  421.4927  115.66667   276.2600   95.750  140.6300   \n",
       "2      19910301  119.0500  368.1324   86.33333   190.7900   80.800  107.5100   \n",
       "3      19910401  121.9400  387.0115   79.66667   200.2700   77.000  110.7000   \n",
       "4      19910501  125.7900  462.6200   82.80000   199.6400   78.600  114.9600   \n",
       "..          ...       ...       ...        ...        ...      ...       ...   \n",
       "392    20230901  684.4500  576.0000  568.45000  1024.2000  603.600  711.9000   \n",
       "393    20231001  660.3750  656.1875  524.37500   970.3125  556.625  700.1250   \n",
       "394    20231101  629.7500  688.1875  489.75000   919.5625  521.875  713.3125   \n",
       "395    20231201  588.4500  589.7000  494.65000   868.7500  503.700  681.8500   \n",
       "396    20240101  585.4375  509.5000  459.56250   845.3125  493.875  657.5625   \n",
       "\n",
       "        10066    542456     24820  ...  543880+543332+542784+530997+530785  \\\n",
       "0     268.750  204.9100   312.130  ...                        33122.130000   \n",
       "1     280.000  136.6500   307.280  ...                        56664.570000   \n",
       "2     199.200  106.0300   226.210  ...                        35121.180000   \n",
       "3     191.375  108.9800   235.200  ...                        19576.480000   \n",
       "4     197.200  112.9100   234.610  ...                        34175.280000   \n",
       "..        ...       ...       ...  ...                                 ...   \n",
       "392  1054.800  659.7000  1022.900  ...                        30863.310778   \n",
       "393  1014.750  664.6875   973.000  ...                        42638.283715   \n",
       "394   936.125  686.7500   880.875  ...                        61926.342563   \n",
       "395   881.800  615.7000   804.650  ...                        52011.115747   \n",
       "396   890.375  611.7500   833.125  ...                        44979.617637   \n",
       "\n",
       "     535034+535038+542800+543348+543896  543924+543376+542828+535086  \\\n",
       "0                          45839.530000                 52842.860000   \n",
       "1                          73207.630000                 89338.320000   \n",
       "2                          48163.420000                 55941.780000   \n",
       "3                          30092.690000                 31844.400000   \n",
       "4                          47063.810000                 54475.450000   \n",
       "..                                  ...                          ...   \n",
       "392                        25898.544058                 48481.681142   \n",
       "393                        50128.701163                 63854.584045   \n",
       "394                        67212.546340                 87067.677940   \n",
       "395                        59195.962123                 70954.699113   \n",
       "396                        53176.273842                 76490.177393   \n",
       "\n",
       "     542832+542696+543380+543928  543932+542700+542836+543384  \\\n",
       "0                   48596.370000                 49887.600000   \n",
       "1                   80358.930000                 85764.820000   \n",
       "2                   51293.410000                 52934.020000   \n",
       "3                   30321.090000                 29244.860000   \n",
       "4                   50017.240000                 51492.530000   \n",
       "..                           ...                          ...   \n",
       "392                 45368.326005                 50994.194898   \n",
       "393                 65398.069490                 70789.307587   \n",
       "394                 82487.096193                 89553.720260   \n",
       "395                 67922.393093                 71746.666697   \n",
       "396                 73701.441735                 77820.435563   \n",
       "\n",
       "     547724+543360+547728+543908+547720+542812+547716+531033  \\\n",
       "0                                         60081.570000         \n",
       "1                                         81404.760000         \n",
       "2                                         52744.530000         \n",
       "3                                         36829.640000         \n",
       "4                                         58368.520000         \n",
       "..                                                 ...         \n",
       "392                                       55121.678414         \n",
       "393                                       65391.097779         \n",
       "394                                       88069.251739         \n",
       "395                                       72573.820342         \n",
       "396                                       76757.290364         \n",
       "\n",
       "     543888+542792+543340+531025  \\\n",
       "0                   39845.120000   \n",
       "1                   68741.690000   \n",
       "2                   42298.800000   \n",
       "3                   23218.850000   \n",
       "4                   41137.780000   \n",
       "..                           ...   \n",
       "392                 32958.539815   \n",
       "393                 45001.265013   \n",
       "394                 65533.770453   \n",
       "395                 54569.016270   \n",
       "396                 50301.784130   \n",
       "\n",
       "     543372+543368+543920+543916+542692+542688+542824+542820  \\\n",
       "0                                         52658.090000         \n",
       "1                                         80129.440000         \n",
       "2                                         54990.750000         \n",
       "3                                         36851.860000         \n",
       "4                                         53887.000000         \n",
       "..                                                 ...         \n",
       "392                                       54409.884701         \n",
       "393                                       72728.810396         \n",
       "394                                       87299.379274         \n",
       "395                                       73118.989011         \n",
       "396                                       77983.329021         \n",
       "\n",
       "     543364+543912+542684+542816  \\\n",
       "0                   31296.190000   \n",
       "1                   54080.590000   \n",
       "2                   33230.870000   \n",
       "3                   18186.700000   \n",
       "4                   32315.440000   \n",
       "..                           ...   \n",
       "392                 36524.003032   \n",
       "393                 49016.415023   \n",
       "394                 60576.883930   \n",
       "395                 47718.064695   \n",
       "396                 51288.683635   \n",
       "\n",
       "     8852013+8852220+10743960+10778824+10778836+10743901+10778899+10779132  \n",
       "0                                             13.03000                      \n",
       "1                                             13.15000                      \n",
       "2                                             13.20000                      \n",
       "3                                             13.51000                      \n",
       "4                                             13.05000                      \n",
       "..                                                 ...                      \n",
       "392                                           11.51250                      \n",
       "393                                           11.68875                      \n",
       "394                                           11.88625                      \n",
       "395                                           11.77250                      \n",
       "396                                           11.73250                      \n",
       "\n",
       "[397 rows x 464 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Sub_Oil_VLCC_Monthly.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "43a4a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "features = ['542236', '67321', '549295', '41108', '541982']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e0d5d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "rr_hparams = {\n",
    "    'alpha': 1,\n",
    "    'solver': 'auto',\n",
    "    'tol': 0.001,\n",
    "    'max_iter': 100,\n",
    "}\n",
    "\n",
    "mlp_hparams = {\n",
    "    'hidden_layer_sizes': (50, 50, 50, 50),\n",
    "    'activation': 'relu',\n",
    "    #'solver': 'adam',\n",
    "    'max_iter': 50,\n",
    "    #'verbose': 10,\n",
    "    'learning_rate_init': .1,\n",
    "}\n",
    "\n",
    "gb_hparams = {\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "}\n",
    "\n",
    "xgb_hparams = {\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "    'random_state':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b3bc5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models/Algorithms used\n",
    "models = {\n",
    "    #'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(**rr_hparams),\n",
    "    'MLP': MLPRegressor(**mlp_hparams),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(**gb_hparams),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'XGBoost': XGBRegressor(**xgb_hparams)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e10684ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_test, best_model, md):\n",
    "    plt.plot(y_test, label='Actual')\n",
    "    plt.plot(best_model.predict(md), label='Predicted')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Target Value')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "aac2b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:24] WARNING: /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MLP', 'Gradient Boosting', 'Random Forest', 'XGBoost'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:24] WARNING: /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MLP', 'Gradient Boosting', 'Random Forest', 'XGBoost'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:24] WARNING: /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MLP', 'Gradient Boosting', 'Random Forest', 'XGBoost'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:25] WARNING: /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MLP', 'Gradient Boosting', 'Random Forest', 'XGBoost'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['MLP', 'Gradient Boosting', 'Random Forest', 'XGBoost'])\n",
      "The best model is: MLP with an accuracy of 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:25] WARNING: /Users/runner/work/xgboost/xgboost/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Loop through each feature\n",
    "for feature in features:\n",
    "    X = data[feature].values.reshape(-1, 1)\n",
    "    y = data[feature].values\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train = X[:-36]  # Select all rows except the last 36\n",
    "    y_train = y[:-36]\n",
    "    X_test = X[-36:]   # Select the last 36 rows\n",
    "    y_test = y[-36:]\n",
    "    \n",
    "    # Pre-processing Technique: Dimensionality Recution - Principal Component Analysis(PCA)\n",
    "    # Standardize the features\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=1)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)  \n",
    "\n",
    "    accuracies = {}\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = 100 * (1 - np.abs((y_test - y_pred) / y_test)).mean()\n",
    "        accuracies[name] = accuracy\n",
    "        \n",
    "    print(accuracies.keys())\n",
    "\n",
    "    # Print accuracies for each model\n",
    "#    print(f\"Feature: {feature}\")\n",
    "#    for name, accuracy in accuracies.items():\n",
    "#        print(f\"{name}: Accuracy = {accuracy:.2f}%\")\n",
    "\n",
    "#    # Visualize the results\n",
    "#    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "#    plt.plot(data.index, data[feature], label='Actual') # Check if it is really 'feature' to put in data[]\n",
    "    \n",
    "#    for name, model in models.items():\n",
    "#        model.fit(X_train, y_train)\n",
    "#        y_pred = model.predict(X_test)\n",
    "#        plt.plot(data.index[36:], y_pred, label=name)\n",
    "#    plt.xlabel('Time')\n",
    "#    plt.ylabel('Target')\n",
    "#    plt.title(f'Predictions for Feature: {feature}')\n",
    "#    plt.legend()\n",
    "#    plt.show()\n",
    "    \n",
    "# Identify the best model based on accuracy\n",
    "best_model = max(accuracies, key=accuracies.get)\n",
    "print(f\"The best model is: {best_model} with an accuracy of {accuracies[best_model]:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2aeff253",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "MLPRegressor(hidden_layer_sizes=(50, 50, 50, 50), learning_rate_init=0.1,\n             max_iter=50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     doc\u001b[38;5;241m.\u001b[39msave(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mplot_actual_vs_predicted\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_36_months\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[238], line 10\u001b[0m, in \u001b[0;36mplot_actual_vs_predicted\u001b[0;34m(y_test, best_model, X_test, filename, last_36_months)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_actual_vs_predicted\u001b[39m(y_test, best_model, X_test, filename, last_36_months):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Save the plot to an Excel file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m: last_36_months,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget Values\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test, \n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Values\u001b[39m\u001b[38;5;124m'\u001b[39m: best_model\u001b[38;5;241m.\u001b[39mpredict(X_test),\n\u001b[0;32m---> 10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43maccuracies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m         })\n\u001b[1;32m     12\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_excel(filename \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Save the plot to a Word document\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: MLPRegressor(hidden_layer_sizes=(50, 50, 50, 50), learning_rate_init=0.1,\n             max_iter=50)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "def plot_actual_vs_predicted(y_test, best_model, X_test, filename, last_36_months):\n",
    "    # Save the plot to an Excel file\n",
    "    df = pd.DataFrame({\n",
    "        'Year': last_36_months,\n",
    "        'Target Values': y_test, \n",
    "        'Predicted Values': best_model.predict(X_test),\n",
    "        })\n",
    "    df.to_excel(filename + \".xlsx\", index=False)\n",
    "\n",
    "    # Save the plot to a Word document\n",
    "    doc = Document()\n",
    "    doc.add_heading('Actual vs Predicted Values', level=1)\n",
    "    doc.add_paragraph(\"Actual vs Predicted Values:\")\n",
    "    doc.add_table(df.shape[0]+1, df.shape[1], style=\"Table Grid\")\n",
    "    table = doc.tables[0]\n",
    "    for i, column in enumerate(df.columns):\n",
    "        table.cell(0, i).text = column\n",
    "        for j, value in enumerate(df[column]):\n",
    "            table.cell(j+1, i).text = str(value)\n",
    "    doc.save(filename + \".docx\")\n",
    "\n",
    "# Example usage\n",
    "plot_actual_vs_predicted(y_test, models[best_model], X_test, \"result\", last_36_months)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
